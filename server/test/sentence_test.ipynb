{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/fstrauf/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#for dataframe manipulation\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "#regular expressoin toolkit\n",
    "import re\n",
    "\n",
    "#NLP toolkits\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#for plotting expense categories later\n",
    "import matplotlib.pyplot as plt \n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.ticker as ticker # for formatting major units on x-y axis\n",
    "\n",
    "#for downloading BERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#for finding most similar text vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a58a2a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_BERT(text):\n",
    "\n",
    "    # Convert words to lower case.\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and numbers. This also removes the dates \n",
    "    # which are not important in classifying expenses\n",
    "    text = re.sub(r'[^\\w\\s]|https?://\\S+|www\\.\\S+|https?:/\\S+|[^\\x00-\\x7F]+|\\d+', '', str(text).strip())\n",
    "  \n",
    "    # Tokenise \n",
    "    text_list = word_tokenize(text)\n",
    "    result = ' '.join(text_list)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b96a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                            description    category\n",
      "0   1300       Ben Jerrys Noosa Noosa Heads QLD  DinnerBars\n",
      "1   1302         LIFELINE NOOSA NOOSA HEADS QLD    Shopping\n",
      "2   1305      LIVELIFE PHRMCY NOOSA NOOSA HEADS   Groceries\n",
      "3   1306                      COLES NOOSA HEADS   Groceries\n",
      "4   1307          FRASER COAST REGIONAL TORQUAY      Travel\n",
      "5   1308      THE DECK SEA SALT RAINBOW BEACQLD  DinnerBars\n",
      "6   1309          RAINBOW BEACH RAINBOW BEACQLD      Travel\n",
      "7   1312                  TELSTRA MELBOURNE VIC     Utility\n",
      "8   1313       COOLUM HOLIDAY PARK COOLUM BEACH      Travel\n",
      "9   1314             GELATO MIO COOLUM BEACHQLD  DinnerBars\n",
      "10  1315    BROKEN HEAD HOLIDAY BROKEN HEAD NSW      Travel\n",
      "11  1316     COFFEE CLUB COOLUM COOLUM BEACHQLD  DinnerBars\n",
      "12  1317                     COLES COOLUM BEACH   Groceries\n",
      "13  1318       FRASER ISLAND FUELS RAINBOW BEAC   Transport\n",
      "14  1319                       KMART LOGANHOLME    Shopping\n",
      "15  1320      KINGSLCIFF NORTH HOLID KINGSCLIFF      Travel\n",
      "16  1321  ORIGINKEBABS HYPERDOME LOGANHOLME QLD  DinnerBars\n",
      "17  1322  The Coffee Club Loganh Loganholme QLD  DinnerBars\n",
      "18  1323    EARTH MARKETS LOGANQPS SHAILER PARK  DinnerBars\n",
      "19  1324         CAESARS BARBER SHOP LOGANHOLME    Shopping\n",
      "20  1326         FREEDOM FUELS TRQPS KINGSCLIFF   Transport\n",
      "21  1330       REFLECTIONS HOLIDAY NAMBUCCA HEA      Travel\n",
      "22  1333        REFLECTIONS HOLIDAY SCOTTS HEAD      Travel\n",
      "23  1334        BEACH HOTEL BYRON BAY BYRON BAY  DinnerBars\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the file path or URL\n",
    "new_file_path = 'new_data.csv'\n",
    "trained_file_path = 'trained_data.csv'\n",
    "\n",
    "# Read the CSV data\n",
    "new_df = pd.read_csv(new_file_path)\n",
    "trained_df = pd.read_csv(trained_file_path)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f54f6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_raw = trained_df['description']\n",
    "trained_text_BERT = text_raw.apply(lambda x: clean_text_BERT(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b00b421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 1.75MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 574kB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 24.9MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<00:00, 1.83MB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 294kB/s]\n",
      "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 522kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:42<00:00, 10.3MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 116kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 863kB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.57MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 2.24MB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 16.2MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 3.16MB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<00:00, 4.16MB/s]\n",
      "Batches: 100%|██████████| 10/10 [00:01<00:00,  5.72it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_input = trained_text_BERT.tolist()\n",
    "model = SentenceTransformer('all-mpnet-base-v2') \n",
    "embeddings = model.encode(bert_input, show_progress_bar = True)\n",
    "embedding_BERT = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808f66ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load texts\n",
    "text_test_raw = new_df['description']\n",
    "\n",
    "# Apply data cleaning function as for training data\n",
    "text_test_BERT = text_test_raw.apply(lambda x: clean_text_BERT(x))\n",
    "\n",
    "\n",
    "# Apply BERT embedding\n",
    "bert_input_test = text_test_BERT.tolist()\n",
    "#model = SentenceTransformer('paraphrase-mpnet-base-v2') \n",
    "embeddings_test = model.encode(bert_input_test, show_progress_bar = True)\n",
    "embedding_BERT_test = np.array(embeddings_test)\n",
    "\n",
    "df_embedding_bert_test = pd.DataFrame(embeddings_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07682619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar word embedding with unseen data in the training data\n",
    "\n",
    "similarity_new_data = cosine_similarity(embedding_BERT_test, embedding_BERT)\n",
    "similarity_df = pd.DataFrame(similarity_new_data)\n",
    "\n",
    "# Returns index for most similar embedding\n",
    "# See first column of the output dataframe below\n",
    "index_similarity = similarity_df.idxmax(axis = 1)\n",
    "\n",
    "# Return dataframe for most similar embedding/transactions in training dataframe\n",
    "data_inspect = trained_df.iloc[index_similarity, :].reset_index(drop = True)\n",
    "\n",
    "unseen_verbatim = text_test_raw\n",
    "matched_verbatim = data_inspect['description']\n",
    "annotation = data_inspect['category']\n",
    "\n",
    "d_output = {\n",
    "            'unseen_transaction': unseen_verbatim,\n",
    "            'matched_transaction': matched_verbatim, \n",
    "            'matched_class': annotation\n",
    "            \n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d7b5990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'unseen_transaction': 0          Ben Jerrys Noosa Noosa Heads QLD\n",
      "1            LIFELINE NOOSA NOOSA HEADS QLD\n",
      "2         LIVELIFE PHRMCY NOOSA NOOSA HEADS\n",
      "3                         COLES NOOSA HEADS\n",
      "4             FRASER COAST REGIONAL TORQUAY\n",
      "5         THE DECK SEA SALT RAINBOW BEACQLD\n",
      "6             RAINBOW BEACH RAINBOW BEACQLD\n",
      "7                     TELSTRA MELBOURNE VIC\n",
      "8          COOLUM HOLIDAY PARK COOLUM BEACH\n",
      "9                GELATO MIO COOLUM BEACHQLD\n",
      "10      BROKEN HEAD HOLIDAY BROKEN HEAD NSW\n",
      "11       COFFEE CLUB COOLUM COOLUM BEACHQLD\n",
      "12                       COLES COOLUM BEACH\n",
      "13         FRASER ISLAND FUELS RAINBOW BEAC\n",
      "14                         KMART LOGANHOLME\n",
      "15        KINGSLCIFF NORTH HOLID KINGSCLIFF\n",
      "16    ORIGINKEBABS HYPERDOME LOGANHOLME QLD\n",
      "17    The Coffee Club Loganh Loganholme QLD\n",
      "18      EARTH MARKETS LOGANQPS SHAILER PARK\n",
      "19           CAESARS BARBER SHOP LOGANHOLME\n",
      "20           FREEDOM FUELS TRQPS KINGSCLIFF\n",
      "21         REFLECTIONS HOLIDAY NAMBUCCA HEA\n",
      "22          REFLECTIONS HOLIDAY SCOTTS HEAD\n",
      "23          BEACH HOTEL BYRON BAY BYRON BAY\n",
      "Name: description, dtype: object, 'matched_transaction': 0        Beach Burrito Company Dee Why NSW\n",
      "1          LIFELINE NORTHERN BALGOWLAH NSW\n",
      "2        SNAK SHAK BROOMS HEAD BROOMS HEAD\n",
      "3                            COLES GOROKAN\n",
      "4     COFFS HARBOUR FISHER COFFS HARBOUNSW\n",
      "5     COAST LIGHTHOUSE BCH LIGHTHOUSE BNSW\n",
      "6     COAST LIGHTHOUSE BCH LIGHTHOUSE BNSW\n",
      "7                    TELSTRA MELBOURNE VIC\n",
      "8               CALYPSO HOLIDAY PARK YAMBA\n",
      "9             Gelato Matteo Freshwater NSW\n",
      "10        NORAH HEAD HOLIDY NORAH HEAD NSW\n",
      "11       Chillax Espresso Bar Collaroy NSW\n",
      "12                         COLES BROOKVALE\n",
      "13    COAST LIGHTHOUSE BCH LIGHTHOUSE BNSW\n",
      "14                        KMART WARRIEWOOD\n",
      "15    COFFS HARBOUR FISHER COFFS HARBOUNSW\n",
      "16                  SPOTTO NSW EAST SYDNEY\n",
      "17           FLYING FOX CAFE MONA VALE NSW\n",
      "18        WESTFIELD WARRINGAHQPS BROOKVALE\n",
      "19       CONVENIENCE STORE DARLINGHURSTNSW\n",
      "20    METRO PETROLEUM FORE FORESTVILLE NSW\n",
      "21                     OEH ARAKOON ARAKOON\n",
      "22        MIDDLE ROCK HOLIDAY ONE MILE NSW\n",
      "23                       Royce Hotel Kilda\n",
      "Name: description, dtype: object, 'matched_class': 0     DinnerBars\n",
      "1       Shopping\n",
      "2     DinnerBars\n",
      "3      Groceries\n",
      "4     DinnerBars\n",
      "5     DinnerBars\n",
      "6     DinnerBars\n",
      "7         Travel\n",
      "8         Travel\n",
      "9     DinnerBars\n",
      "10        Travel\n",
      "11    DinnerBars\n",
      "12     Groceries\n",
      "13    DinnerBars\n",
      "14      Shopping\n",
      "15    DinnerBars\n",
      "16      Business\n",
      "17    DinnerBars\n",
      "18      Shopping\n",
      "19     Charlotte\n",
      "20     Transport\n",
      "21        Travel\n",
      "22      Shopping\n",
      "23      Business\n",
      "Name: category, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "print(d_output)\n",
    "d_output\n",
    "df = pd.DataFrame(d_output)\n",
    "\n",
    "# Export the DataFrame to a CSV file\n",
    "df.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
